{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0aa209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/cross-entropy-negative-log-likelihood-and-all-that-jazz-47a95bd2e81\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5892f98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------Setting up binary case-----------------------------\n",
      "z=tensor([0.6400, 0.2700, 0.0400, 0.0200, 0.8100])\n",
      "yhat=tensor([0.6548, 0.5671, 0.5100, 0.5050, 0.6921])\n",
      "y=tensor([1., 0., 0., 1., 0.])\n",
      "--------------------------------------------------------------------------------\n",
      "yhat.log():  tensor([-0.4235, -0.5672, -0.6733, -0.6832, -0.3680])\n",
      "(1 - yhat).log()):  tensor([-1.0635, -0.8372, -0.7133, -0.7032, -1.1780])\n",
      "y * yhat.log():  tensor([-0.4235, -0.0000, -0.0000, -0.6832, -0.0000])\n",
      "(1 - y) * (1 - yhat).log():  tensor([-0.0000, -0.8372, -0.7133, -0.0000, -1.1780])\n",
      "tensor([0.4235, 0.8372, 0.7133, 0.6832, 1.1780])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Binary setting ##############################################################\n",
    "print(f\"{'Setting up binary case':-^80}\")\n",
    "z = torch.tensor([0.64, 0.27, 0.04, 0.02, 0.81])\n",
    "yhat = torch.sigmoid(z)\n",
    "#yhat = z\n",
    "y = torch.Tensor([1,0,0,1,0])\n",
    "\n",
    "print(f\"{z=}\\n{yhat=}\\n{y=}\\n{'':-^80}\")\n",
    "print(\"yhat.log(): \", yhat.log())\n",
    "print(\"(1 - yhat).log()): \",(1 - yhat).log())\n",
    "# First compute the negative log likelihoods using the derived formula\n",
    "print(\"y * yhat.log(): \",y * yhat.log())\n",
    "print(\"(1 - y) * (1 - yhat).log(): \",(1 - y) * (1 - yhat).log())\n",
    "\n",
    "l = -(y * yhat.log() + (1 - y) * (1 - yhat).log())\n",
    "print(f\"{l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16f2e644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4235, 0.8372, 0.7133, 0.6832, 1.1780])\n",
      "tensor([0.4235, 0.8372, 0.7133, 0.6832, 1.1780])\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Observe that BCELoss and BCEWithLogitsLoss can produce the same results\n",
    "\n",
    "# Supposing: z: prediction , s: sigmoid(z) , \n",
    "l_BCELoss_nored = torch.nn.BCELoss(reduction=\"none\")(yhat, y) # log(s)\n",
    "l_BCEWithLogitsLoss_nored = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(z, y) # log(z)\n",
    "print(f\"{l_BCELoss_nored}\\n{l_BCEWithLogitsLoss_nored}\\n{'':=^80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Binary setting ##############################################################\n",
    "print(f\"{'Setting up binary case':-^80}\")\n",
    "z = torch.randn(5)\n",
    "yhat = torch.sigmoid(z)\n",
    "y = torch.Tensor([0, 1, 1, 0, 1])\n",
    "print(f\"{z=}\\n{yhat=}\\n{y=}\\n{'':-^80}\")\n",
    "\n",
    "# First compute the negative log likelihoods using the derived formula\n",
    "l = -(y * yhat.log() + (1 - y) * (1 - yhat).log())\n",
    "print(f\"{l}\")\n",
    "\n",
    "# Observe that BCELoss and BCEWithLogitsLoss can produce the same results\n",
    "l_BCELoss_nored = torch.nn.BCELoss(reduction=\"none\")(yhat, y)\n",
    "l_BCEWithLogitsLoss_nored = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(z, y)\n",
    "print(f\"{l_BCELoss_nored}\\n{l_BCEWithLogitsLoss_nored}\\n{'':=^80}\")\n",
    "\n",
    "# Multiclass setting ##########################################################\n",
    "print(f\"{'Setting up multiclass case':-^80}\")\n",
    "z2 = torch.randn(5, 3)\n",
    "yhat2 = torch.softmax(z2, dim=-1)\n",
    "y2 = torch.Tensor([0, 2, 1, 1, 0]).long()\n",
    "print(f\"{z2=}\\n{yhat2=}\\n{y2=}\\n{'':-^80}\")\n",
    "\n",
    "# First compute the negative log likelihoods using the derived formulat\n",
    "l2 = -yhat2.log()[torch.arange(5), y2]  # masking the correct entries\n",
    "print(f\"{l2}\")\n",
    "print(-torch.log_softmax(z2, dim=-1)[torch.arange(5), y2])\n",
    "\n",
    "# Observe that NLLLoss and CrossEntropyLoss can produce the same results\n",
    "l2_NLLLoss_nored = torch.nn.NLLLoss(reduction=\"none\")(yhat2.log(), y2)\n",
    "l2_CrossEntropyLoss_nored = torch.nn.CrossEntropyLoss(reduction=\"none\")(z2, y2)\n",
    "print(f\"{l2_NLLLoss_nored}\\n{l2_CrossEntropyLoss_nored}\\n{'':=^80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
