{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0eca5fe4-7cff-43b4-872c-3c5ea841e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://machinelearningmastery.com/the-attention-mechanism-from-scratch/\n",
    "from numpy import array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a57ebb08-1a5e-4cd8-9de6-8bb6a1ae4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder representations of four different words\n",
    "word_1 = array([1, 0, 0])\n",
    "word_2 = array([0, 1, 0])\n",
    "word_3 = array([1, 1, 0])\n",
    "word_4 = array([0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e39c2b5-ba5a-4db5-8b87-ac900df92d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "# generating the weight matrices\n",
    "random.seed(42) # to allow us to reproduce the same attention values\n",
    "W_Q = random.randint(3, size=(3, 3))\n",
    "W_K = random.randint(3, size=(3, 3))\n",
    "W_V = random.randint(3, size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3efb1d1a-872a-4a7c-8bb2-03d42bbf9167",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# generating the queries, keys and values\n",
    "query_1 = word_1 @ W_Q\n",
    "key_1 = word_1 @ W_K\n",
    "value_1 = word_1 @ W_V\n",
    "\n",
    "query_2 = word_2 @ W_Q\n",
    "key_2 = word_2 @ W_K\n",
    "value_2 = word_2 @ W_V\n",
    "\n",
    "query_3 = word_3 @ W_Q\n",
    "key_3 = word_3 @ W_K\n",
    "value_3 = word_3 @ W_V\n",
    "\n",
    "query_4 = word_4 @ W_Q\n",
    "key_4 = word_4 @ W_K\n",
    "value_4 = word_4 @ W_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "292fb030-e286-4567-b34c-8ff5b7c75154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "# scoring the first query vector against all key vectors\n",
    "scores = array([dot(query_1, key_1), dot(query_1, key_2), dot(query_1, key_3), dot(query_1, key_4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb9ef7ff-16d9-4cdf-bb34-c615b39bf6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8,  2, 10,  2]),\n",
       " array([2, 0, 2]),\n",
       " array([2, 4, 3]),\n",
       " array([2, 0, 2]),\n",
       " array([0, 1, 1]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores,query_1, key_3,query_1, key_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e28d9fa-cf6b-4d3f-ba42-df6ee09df7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the weights by a softmax operation\n",
    "import torch\n",
    "tscores = torch.tensor([scores]).float()\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "weights= m(tscores)\n",
    "weights=weights[0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a46ff368-7c2a-4ee4-9085-f28d3ebfc4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9994, 1.8800, 0.8806], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# computing the attention by a weighted sum of the value vectors\n",
    "attention = (weights[0] * value_1) + (weights[1] * value_2) + (weights[2] * value_3) + (weights[3] * value_4)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0e1b8edb-69df-49a0-9fe4-83b2c1841e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98522025 1.74174051 0.75652026]\n",
      " [0.90965265 1.40965265 0.5       ]\n",
      " [0.99851226 1.75849334 0.75998108]\n",
      " [0.99560386 1.90407309 0.90846923]]\n",
      "[[ 8  2 10  2]\n",
      " [ 4  0  4  0]\n",
      " [12  2 14  2]\n",
      " [10  4 14  3]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import random\n",
    "from numpy import dot\n",
    "from scipy.special import softmax\n",
    "\n",
    "# encoder representations of four different words\n",
    "word_1 = array([1, 0, 0])\n",
    "word_2 = array([0, 1, 0])\n",
    "word_3 = array([1, 1, 0])\n",
    "word_4 = array([0, 0, 1])\n",
    "\n",
    "# stacking the word embeddings into a single array\n",
    "words = array([word_1, word_2, word_3, word_4])\n",
    "\n",
    "# generating the weight matrices\n",
    "random.seed(42)\n",
    "W_Q = random.randint(3, size=(3, 3))\n",
    "W_K = random.randint(3, size=(3, 3))\n",
    "W_V = random.randint(3, size=(3, 3))\n",
    "\n",
    "# generating the queries, keys and values\n",
    "Q = words @ W_Q #query #  st_1: previous decoder output  st-1      \n",
    "K = words @ W_K #key#hi#  hi, encoded  hidden state. K.shape is [4,3]. since [4,3]*[3,3]\n",
    "V = words @ W_V #values#  hi\n",
    "\n",
    "# scoring the query vectors against all key vectors\n",
    "scores = Q @ K.transpose() # alignment scores K.transpose().shape is [3,4]\n",
    "###########My dscoveries###############\n",
    "'''\n",
    "Notation: \n",
    "a: length of words sequence, in this case:4. \n",
    "b: length of each word. In this case:3\n",
    "c: row length of W_Q,W_K,W_V\n",
    "d: colum length of W_Q,W_K,W_V\n",
    "\n",
    "Discoveries 0: row lenth of W_Q,W_K,W_V always == b, \n",
    "               so b==c\n",
    "\n",
    "Discoveries 1: scores is  always a square matrix, with shape a*a\n",
    "    Why:   words.shape: a*b, W_Q.shape: c*d\n",
    "        so     :Q.shape: a*d, K.shape: a*d, K_transpose().shape d*a\n",
    "        so     :Q.shape*K_transpose().shape = a*a\n",
    "        finally:scores.shape==a*a\n",
    "\n",
    "'''\n",
    "#######################################\n",
    "\n",
    "# computing the weights by a softmax operation\n",
    "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n",
    "\n",
    "# computing the attention by a weighted sum of the value vectors\n",
    "attention = weights @ V\n",
    "###########My dscoveries###############\n",
    "'''\n",
    "Notation: \n",
    "a: length of words sequence, in this case:4. \n",
    "b: length of each word. In this case:3\n",
    "c: row length of W_Q,W_K,W_V\n",
    "d: colum length of W_Q,W_K,W_V\n",
    "\n",
    "Discoveries 2: attention is  always a matrix with shape a*d\n",
    "    Why:   words.shape: a*b, W_V.shape: c*d\n",
    "        so     :V.shape: a*d, scores.shape==weights.shape: a*a\n",
    "        so     :weights.shape*V.shape = a*d\n",
    "        finally:attention.shape==a*d\n",
    "'''\n",
    "#######################################\n",
    "print(attention)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "58c0c0ff-0985-4269-a13c-0170101e5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "373fa039-e4f8-449c-a7fd-cc561bd16fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.36089863e-01, 7.38987555e-03, 7.49130386e-01, 7.38987555e-03],\n",
       "       [4.54826323e-01, 4.51736775e-02, 4.54826323e-01, 4.51736775e-02],\n",
       "       [2.39275049e-01, 7.43870015e-04, 7.59237211e-01, 7.43870015e-04],\n",
       "       [8.99501754e-02, 2.81554063e-03, 9.05653685e-01, 1.58059922e-03]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "000fdfa5-7e08-4235-b0b4-177d723fee17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e456449-f017-48c8-9733-6e3f11dd9f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a41249a-ce49-4e8c-94bf-2b719ec3c47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90a297-a866-4ed9-8fc0-ab0e3fd56914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
