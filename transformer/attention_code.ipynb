{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://peterbloem.nl/blog/transformers\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 14,  32],\n",
      "         [ 32,  77]],\n",
      "\n",
      "        [[194, 266],\n",
      "         [266, 365]]])\n"
     ]
    }
   ],
   "source": [
    "# Assume we have some tensor x with size (b, t, k)\n",
    "x = [[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]]\n",
    "\"\"\"\n",
    "x format: \n",
    "   b1       b2\n",
    "|1,2,3| |7 ,8 ,9 | \n",
    "|4,5,6| |10,11,12| \n",
    "\n",
    "x.transpose format: \n",
    " b1        b2\n",
    "|1,4| |7,10,11,12|\n",
    "|2,5| |8,10,11,12|\n",
    "|3,6| |9,10,11,12|\n",
    "\"\"\"\n",
    "\n",
    "xt=torch.tensor(x)\n",
    "xt.transpose(1,2)\n",
    "raw_weights = torch.bmm(xt, xt.transpose(1, 2))\n",
    "print(raw_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([[[1.5230e-08, 1.0000e+00],\n",
      "         [2.8625e-20, 1.0000e+00]],\n",
      "\n",
      "        [[5.3802e-32, 1.0000e+00],\n",
      "         [1.0089e-43, 1.0000e+00]]])\n",
      "y:  tensor([[[ 4.,  5.,  6.],\n",
      "         [ 4.,  5.,  6.]],\n",
      "\n",
      "        [[10., 11., 12.],\n",
      "         [10., 11., 12.]]])\n"
     ]
    }
   ],
   "source": [
    "weights = F.softmax(raw_weights.float(), dim=2)\n",
    "print(\"weights: \",weights)\n",
    "y = torch.bmm(weights, xt.float())\n",
    "print(\"y: \",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three k√ók weight matrices ùêñq, ùêñk,ùêñv and compute three linear transformations of each xi,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: \n",
      " tensor([[-0.1837,  0.1492, -0.4361, -0.8322,  1.1574, -0.0720,  0.5413, -0.0395,\n",
      "          0.6671, -1.2583]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "k=10\n",
    "key = nn.Linear(k, k, bias=False,dtype=torch.float)\n",
    "#x = torch.tensor([1,0,0,0,0,0,0,0,0,0])\n",
    "x = torch.randn(1,10)\n",
    "y= key(x.float())\n",
    "print(\"y: \\n\",y)\n",
    "# for pa in key.parameters():\n",
    "#     print(pa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0626, -0.4637,  0.5218,  0.7086, -0.1552,  1.1626, -0.7062,  1.8758,\n",
      "         -1.2824, -0.4376, -0.7546,  0.5570]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0626, -0.4637],\n",
       "         [ 0.5218,  0.7086]],\n",
       "\n",
       "        [[-0.1552,  1.1626],\n",
       "         [-0.7062,  1.8758]],\n",
       "\n",
       "        [[-1.2824, -0.4376],\n",
       "         [-0.7546,  0.5570]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,12)\n",
    "print(x)\n",
    "x.view(3,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[100.0400,   3.0800],\n",
      "         [  3.0800,   0.2500]]])\n",
      "tensor([[[100.0600,   2.0800],\n",
      "         [  3.1200,   0.2200]]])\n",
      "xt.transpose(1, 2): \n",
      " tensor([[[10.0000,  0.3000],\n",
      "         [ 0.2000,  0.4000]]])\n",
      "xt: \n",
      " tensor([[[10.0000,  0.2000],\n",
      "         [ 0.3000,  0.4000]]])\n"
     ]
    }
   ],
   "source": [
    "xt = torch.tensor([[[10,0.2],[0.3,0.4]]])\n",
    "print(torch.bmm(xt, xt.transpose(1, 2)))\n",
    "print(torch.bmm(xt, xt))\n",
    "print(\"xt.transpose(1, 2): \\n\",xt.transpose(1, 2))\n",
    "print(\"xt: \\n\",xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv tensor([[99,  5],\n",
      "        [ 2,  1],\n",
      "        [ 3,  4]]) matrix: tensor([[99,  5,  2],\n",
      "        [ 1,  3,  4]])\n",
      "mv \n",
      " tensor([[0, 5],\n",
      "        [2, 1],\n",
      "        [3, 4]]) \n",
      "matrix: \n",
      " tensor([[0, 5, 2],\n",
      "        [1, 3, 4]]) \n",
      "mr: \n",
      " tensor([[0, 5],\n",
      "        [2, 1],\n",
      "        [3, 4]])\n",
      "matrix.view(3, 2):  tensor([[0, 1, 5],\n",
      "        [3, 2, 4]])\n",
      "matrix:  tensor([[0, 5, 2],\n",
      "        [1, 3, 4]])\n",
      "tensor([[99,  5,  2],\n",
      "        [ 1,  3,  4]])\n",
      "matrix tensor([[99,  5,  2],\n",
      "        [ 1,  3,  4]])\n"
     ]
    }
   ],
   "source": [
    "#Note the difference between matrix reshaping and the matrix transpose (done with the .t() function):\n",
    "matrix = torch.tensor([[3,5,2],[1, 3, 4]])\n",
    "\n",
    "# view test\n",
    "mv = matrix.view(3, 2)\n",
    "mv[0][0]=99\n",
    "print(\"mv\",mv,\"matrix:\",matrix)\n",
    "\n",
    "# reshape test\n",
    "mr=matrix.contiguous().view((3, 2))\n",
    "mr[0][0]=0\n",
    "print(\"mv \\n\",mv,\"\\nmatrix: \\n\",matrix,\"\\nmr: \\n\",mr )\n",
    "\n",
    "print(\"matrix.view(3, 2): \",(matrix.t().contiguous().view(2,3)))\n",
    "\n",
    "print(\"matrix: \", matrix)\n",
    "mv = matrix.view(2,3)\n",
    "mv[0][0] = 99\n",
    "print(mv)\n",
    "print(\"matrix\",matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward compute of attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4, 10])\n",
      "fkeys.size() torch.Size([4, 2, 10])\n",
      "fkeys.transpose(1, 2).size(): torch.Size([4, 10, 2])\n",
      "dot.size() torch.Size([4, 2, 2])\n",
      "\n",
      " dot size:  torch.Size([4, 2, 2])\n",
      "tensor([ 3.0379e-01,  1.1340e+00, -5.7817e-02, -5.4739e-04, -7.4721e-02,\n",
      "        -5.8388e-01,  2.6483e-01, -1.5910e-01,  1.1531e-01, -2.3734e-01],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0.4273, 0.5727], grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.1169,  0.5041, -0.3150,  0.7244, -0.0101, -0.4413,  0.1939,  0.0449,\n",
      "        -0.2114, -0.0252], grad_fn=<SelectBackward0>)\n",
      "out.size():  torch.Size([1, 2, 40])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,2,40)\n",
    "b,t,k = x.size()\n",
    "h = 4\n",
    "\n",
    "# Create net\n",
    "tokeys    = nn.Linear(k, k, bias=False)\n",
    "toqueries = nn.Linear(k, k, bias=False)\n",
    "tovalues  = nn.Linear(k, k, bias=False)\n",
    "unifyheads = nn.Linear(k, k)\n",
    "\n",
    "# Calculate q,k,v\n",
    "queries = toqueries(x)\n",
    "keys    = tokeys(x)   \n",
    "values  = tovalues(x)\n",
    "\n",
    "s = k // h\n",
    "keys    = keys.view(b, t, h, s)\n",
    "queries = queries.view(b, t, h, s)\n",
    "values  = values.view(b, t, h, s)\n",
    "\n",
    "print(keys.size())\n",
    "\n",
    "# - fold heads into the batch dimension. This ensures that we can use torch.bmm()\n",
    "fkeys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "fqueries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "fvalues = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "print(\"fkeys.size()\",fkeys.size())\n",
    "print(\"fkeys.transpose(1, 2).size():\",fkeys.transpose(1, 2).size())\n",
    "# Get dot product of queries and keys, and scale\n",
    "dot = torch.bmm(fqueries, fkeys.transpose(1, 2))\n",
    "\n",
    "print(\"dot.size()\",dot.size())\n",
    "# -- dot has size (b*h, t, t) containing raw weights\n",
    "# scale the dot product\n",
    "dot = dot / (k ** (1/2))\n",
    "# normalize \n",
    "dot = F.softmax(dot, dim=2)\n",
    "# print(\"dot: \",dot,\"\\n size: \\n\",dot.size())\n",
    "print(\"\\n dot size: \",dot.size())\n",
    "\n",
    "# apply the self attention to the values\n",
    "print(fvalues[0][0])\n",
    "print(dot[0][0])\n",
    "out = torch.bmm(dot, fvalues)\n",
    "print(out[0][0])\n",
    "# swap h, t back, unify heads\n",
    "out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
    "print(\"out.size(): \",out.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
